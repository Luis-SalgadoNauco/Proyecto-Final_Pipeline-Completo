# Proyecto Final â€“ Pipeline de Datos (Semana 3)

## DescripciÃ³n
Este proyecto corresponde al **Mes 3 del curso de AnÃ¡lisis de Datos**, enfocado en **Herramientas ETL y un Proyecto Final**.  
Durante esta semana se diseÃ±a, implementa y documenta un **pipeline de datos completo**, siguiendo buenas prÃ¡cticas de arquitectura, manejo de errores y preparaciÃ³n para producciÃ³n.

El repositorio funciona como una **bitÃ¡cora tÃ©cnica**, donde el README se actualiza diariamente con el avance del proyecto.

---

## Objetivo General de la Semana
- DiseÃ±ar una arquitectura de datos alineada a requisitos de negocio
- Implementar un pipeline de datos end-to-end
- Aplicar manejo de errores y logging
- Validar y probar el pipeline
- Optimizar rendimiento y documentar el sistema

---

## ğŸ› ï¸ TecnologÃ­as utilizadas
- Python
- Pandas
- PostgreSQL
- Apache Airflow
- Linux (Ubuntu)
- Git / GitHub
- Markdown para documentaciÃ³n

---

## Desarrollo de la semana

### DÃ­a 1 â€“ DiseÃ±o de Arquitectura Completa

#### Objetivo del dÃ­a
DiseÃ±ar una arquitectura bÃ¡sica de datos para un sistema de analÃ­tica,
definiendo componentes, decisiones tecnolÃ³gicas y criterios de diseÃ±o.

---

#### Actividades realizadas
- IdentificaciÃ³n de componentes principales del sistema
- DefiniciÃ³n del flujo de datos (ingesta â†’ procesamiento â†’ almacenamiento â†’ consumo)
- DocumentaciÃ³n de decisiones arquitectÃ³nicas

---

#### Arquitectura definida
Se definiÃ³ una arquitectura simplificada orientada a analÃ­tica de retail, considerando fuentes de datos, procesamiento, almacenamiento y consumo.

---

#### Decisiones arquitectÃ³nicas
- Base de datos: PostgreSQL por su madurez y facilidad de uso
- OrquestaciÃ³n: Apache Airflow como estÃ¡ndar de la industria
- VisualizaciÃ³n: Herramientas orientadas a negocio

---

#### VerificaciÃ³n â€“ DÃ­a 1

**Â¿QuÃ© factores considerarÃ­as al elegir entre una arquitectura simple vs compleja?**
- Volumen y frecuencia de datos
- Requisitos del negocio
- Presupuesto disponible
- Capacidad y experiencia del equipo
- Escalabilidad futura

**Â¿CÃ³mo comunicar decisiones arquitectÃ³nicas?**
- A equipos tÃ©cnicos: diagramas, flujos y justificaciÃ³n tÃ©cnica
- A stakeholders de negocio: impacto, beneficios y riesgos

---

### DÃ­a 2 â€“ ImplementaciÃ³n del Pipeline End-to-End

#### Objetivo del dÃ­a
Implementar un pipeline bÃ¡sico de datos en Python que represente un flujo de extremo a extremo, incorporando manejo explÃ­cito de errores y logging, dejando la base preparada para una futura orquestaciÃ³n con Airflow.

---

#### Actividades realizadas
- Uso de un entorno virtual (`venv`) para aislar dependencias
- DefiniciÃ³n de un pipeline secuencial con pasos claros
- ImplementaciÃ³n de manejo de errores mediante excepciones controladas
- IncorporaciÃ³n de logging para trazabilidad de ejecuciÃ³n
- EjecuciÃ³n del pipeline y validaciÃ³n de comportamiento ante errores

El cÃ³digo de implementaciÃ³n se encuentra en:
- `src/pipeline.py`
- `src/utils.py`

---

#### DescripciÃ³n del pipeline
El pipeline implementado consta de tres etapas principales:
1. Captura de datos desde una fuente simulada
2. ValidaciÃ³n y limpieza de datos
3. Persistencia de datos procesados

Se simula un error en la etapa de validaciÃ³n para comprobar el correcto manejo de fallos.

---

#### Manejo de errores y logging
El pipeline:
- Detecta errores de forma explÃ­cita
- Registra eventos y excepciones en logs
- Detiene la ejecuciÃ³n de manera controlada ante errores crÃ­ticos

Esto evita resultados incorrectos y facilita el diagnÃ³stico.

---

#### VerificaciÃ³n â€“ DÃ­a 2

**Â¿QuÃ© diferencia hay entre un pipeline que falla silenciosamente y uno con buen manejo de errores?**
Un pipeline con manejo de errores:
- Detecta y registra fallos
- Facilita el mantenimiento
- Protege la calidad de los datos

Un pipeline que falla silenciosamente:
- Oculta errores
- Genera resultados incorrectos
- Dificulta la detecciÃ³n de problemas en producciÃ³n

**Â¿CÃ³mo decidir cuÃ¡ndo reintentar vs abortar una ejecuciÃ³n?**
- Reintentar ante errores transitorios (red, APIs externas)
- Abortar ante errores de validaciÃ³n o lÃ³gica de negocio

---
 
### DÃ­a 3 â€“ ValidaciÃ³n y Pruebas

#### Objetivo del DÃ­a
Implementar pruebas bÃ¡sicas para validar la lÃ³gica del pipeline de datos, asegurando que los cÃ¡lculos crÃ­ticos funcionen correctamente antes de avanzar hacia etapas de optimizaciÃ³n o despliegue.

---

#### Actividades Realizadas

#### IntroducciÃ³n al Testing en Pipelines de Datos
Se revisÃ³ la importancia del testing en pipelines de datos, considerando que estos procesan informaciÃ³n crÃ­tica para el negocio. Las pruebas permiten detectar errores tempranamente y evitar regresiones cuando el cÃ³digo evoluciona.

Se identificaron los principales tipos de pruebas aplicables a pipelines:
- **Pruebas unitarias**
- **Pruebas de integraciÃ³n**
- **ValidaciÃ³n de calidad de datos**

---

#### ImplementaciÃ³n de FunciÃ³n a Testear
Se utilizÃ³ una funciÃ³n de negocio encargada de calcular el total de ventas por producto, considerando cantidad y precio. Esta funciÃ³n forma parte del mÃ³dulo `utils` del proyecto y representa una lÃ³gica crÃ­tica del pipeline.

---

#### CreaciÃ³n de Prueba Unitaria BÃ¡sica
Se creÃ³ una prueba unitaria en la carpeta `tests/`, validando:
- La correcta acumulaciÃ³n de ventas por producto
- El manejo de mÃºltiples registros para un mismo producto

La prueba fue implementada sin frameworks externos, utilizando `assert`, alineado al enfoque del curso y permitiendo una validaciÃ³n clara y directa de la lÃ³gica.

La ejecuciÃ³n de la prueba fue **exitosa**, confirmando el correcto funcionamiento de la funciÃ³n evaluada.

---

#### VerificaciÃ³n â€“ Respuestas

**Â¿Por quÃ© es importante testear pipelines de datos?**

Porque los pipelines procesan datos crÃ­ticos para el negocio. Un error no detectado puede propagarse hasta reportes o dashboards, generando decisiones incorrectas. El testing asegura confiabilidad y calidad en el procesamiento.


**Â¿QuÃ© tipos de errores son mÃ¡s comunes y cÃ³mo detectarlos?**

- **Errores de lÃ³gica**: detectados mediante pruebas unitarias
- **Errores de integraciÃ³n**: detectados al probar componentes en conjunto
- **Errores de calidad de datos**: detectados mediante validaciones de completitud, exactitud y consistencia

---

### DÃ­a 4 â€“ OptimizaciÃ³n y Rendimiento

#### Objetivo del dÃ­a
Identificar posibles cuellos de botella en un pipeline de datos y aplicar tÃ©cnicas bÃ¡sicas de optimizaciÃ³n, evaluando el impacto real mediante mediciones de rendimiento.

---

#### Actividades realizadas
- AnÃ¡lisis conceptual de cuellos de botella en pipelines de datos (ingesta, procesamiento, almacenamiento e I/O).
- ImplementaciÃ³n de mediciÃ³n de tiempo de ejecuciÃ³n para funciones de procesamiento.
- ComparaciÃ³n entre una versiÃ³n secuencial y una versiÃ³n alternativa de procesamiento de datos.
- EjecuciÃ³n de pruebas con diferentes volÃºmenes de datos para evaluar impacto de la optimizaciÃ³n.
- GeneraciÃ³n de evidencia persistente de los resultados obtenidos.

---

#### EvaluaciÃ³n de rendimiento
Se evaluaron dos versiones de una funciÃ³n de procesamiento de datos:
- VersiÃ³n secuencial (baseline).
- VersiÃ³n alternativa con enfoque â€œoptimizadoâ€.

Las pruebas se realizaron con:
- Volumen moderado de datos.
- Volumen grande (1.000.000 de registros).

Los resultados demostraron que:
- Para ciertos escenarios, la versiÃ³n alternativa no presenta mejoras significativas.
- El uso de estructuras y operaciones internas de Python puede introducir overhead adicional.
- La optimizaciÃ³n debe basarse en mÃ©tricas reales y no en supuestos teÃ³ricos.

---

#### Evidencia generada
Los resultados de las pruebas de rendimiento fueron almacenados como evidencia tÃ©cnica en archivos de texto:

- `docs/performance_day4.txt`
- `docs/performance_day4_large.txt`

Estos archivos contienen:
- Cantidad de registros procesados.
- Tiempo de ejecuciÃ³n de cada versiÃ³n.
- ComparaciÃ³n directa de rendimiento.
- ConclusiÃ³n de la prueba.

---

#### VerificaciÃ³n â€“ Respuestas

**Â¿CuÃ¡ndo deberÃ­as optimizar el rendimiento?**  
Cuando existen cuellos de botella identificados mediante mÃ©tricas reales y el impacto justifica el aumento de complejidad.

**Â¿QuÃ© compensaciones considerar entre velocidad y complejidad?**  
Una optimizaciÃ³n puede mejorar tiempos de ejecuciÃ³n, pero tambiÃ©n aumentar la complejidad del cÃ³digo, dificultar el mantenimiento y reducir la legibilidad. Siempre se debe evaluar el balance entre rendimiento, claridad y costo operativo.

---

### DÃ­a 5 â€“ DocumentaciÃ³n y PresentaciÃ³n

#### Objetivo del dÃ­a
Documentar tÃ©cnicamente el pipeline desarrollado durante la semana y preparar una comunicaciÃ³n clara de resultados, considerando distintas audiencias (tÃ©cnica y de negocio).

---

#### DocumentaciÃ³n tÃ©cnica

Se reforzÃ³ la importancia de documentar correctamente los componentes del pipeline para facilitar el mantenimiento, reducir errores y acelerar el onboarding de nuevos integrantes del equipo.

Se documentÃ³ el componente de validaciÃ³n de datos de ventas, incorporando:
- PropÃ³sito claro del componente
- Reglas de validaciÃ³n
- Estructura de salida
- Ejemplo de uso probado en entorno local

El componente `validar_datos_ventas` valida criterios bÃ¡sicos de calidad antes del procesamiento:
- El precio debe ser mayor a 0
- La fecha no puede estar vacÃ­a
- Se reportan errores detallados por fila

La funciÃ³n fue integrada y ejecutada exitosamente desde `src/utils.py`, validando su correcto funcionamiento.

---

#### ComunicaciÃ³n de resultados

Se trabajÃ³ en la estructuraciÃ³n de un **resumen ejecutivo**, orientado a audiencias no tÃ©cnicas, destacando impacto, beneficios y mÃ©tricas clave del proyecto.

Resumen ejecutivo del proyecto:

- **Proyecto:** Pipeline de Analytics de Ventas
- **Objetivo:** Automatizar reportes de ventas semanales
- **SoluciÃ³n implementada:** Pipeline ETL con validaciones de calidad de datos
- **Beneficios principales:**
  - ReducciÃ³n del tiempo de reporte: 3 dÃ­as â†’ 2 horas
  - Mejora en la calidad de datos: 95% de precisiÃ³n
  - Ahorro anual estimado: â‚¬50,000
- **MÃ©tricas clave:**
  - Tiempo de procesamiento: 2 horas
  - PrecisiÃ³n de datos: 95%
  - Disponibilidad del sistema: 99.5%

Este enfoque permite comunicar el valor del pipeline tanto a equipos tÃ©cnicos como a stakeholders de negocio.

---

#### VerificaciÃ³n

**Â¿CÃ³mo adaptar una presentaciÃ³n tÃ©cnica para diferentes audiencias?**

- Audiencia tÃ©cnica: detalles de implementaciÃ³n, validaciones, mÃ©tricas tÃ©cnicas y decisiones de diseÃ±o.
- Audiencia de negocio: impacto, beneficios, ahorro de tiempo y mejora en la toma de decisiones.
- Audiencia ejecutiva: resumen ejecutivo, resultados clave y retorno de inversiÃ³n.

**Â¿QuÃ© elementos son mÃ¡s importantes en la documentaciÃ³n?**

- README: visiÃ³n general y guÃ­a principal del proyecto.
- CÃ³digo comentado y docstrings: comprensiÃ³n tÃ©cnica detallada.
- Diagramas: entendimiento rÃ¡pido de la arquitectura.

La combinaciÃ³n de estos elementos asegura una documentaciÃ³n clara, mantenible y profesional.

---

## Estructura del proyecto

```text
week_3_final_project/
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ architecture_diagram.txt
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_retail.md
â”‚   â”œâ”€â”€ performance_day4.txt
â”‚   â””â”€â”€ performance_day4_large.txt
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”œâ”€â”€ notes/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

